{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_speaker.conv_models import DeepSpeakerModel\n",
    "from glob import glob\n",
    "import os\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from deep_speaker.audio import read_mfcc\n",
    "from deep_speaker.batcher import sample_from_mfcc\n",
    "from deep_speaker.constants import SAMPLE_RATE, NUM_FRAMES\n",
    "from deep_speaker.conv_models import DeepSpeakerModel\n",
    "from deep_speaker.test import batch_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepSpeakerModel()\n",
    "path = '/home/repsmate/saa-sr/checkpoints-triplets/ResCNN_checkpoint_120.h5'\n",
    "\n",
    "# Load the checkpoint. https://drive.google.com/file/d/1F9NvdrarWZNktdX9KlRYWWHDwRkip_aP.\n",
    "# Also available here: https://share.weiyun.com/V2suEUVh (Chinese users).\n",
    "model.m.load_weights(path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = '/home/repsmate/datasets/test'\n",
    "config_path = '/home/repsmate/saa-sr-inference/tests/config.yaml'\n",
    "audio_files = glob(os.path.join(audio_dir, '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_001 = sample_from_mfcc(read_mfcc(audio_files[0], SAMPLE_RATE), NUM_FRAMES)\n",
    "mfcc_002 = sample_from_mfcc(read_mfcc(audio_files[1], SAMPLE_RATE), NUM_FRAMES)\n",
    "predict_001 = model.m.predict(np.expand_dims(mfcc_001, axis=0))\n",
    "predict_002 = model.m.predict(np.expand_dims(mfcc_002, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pos = batch_cosine_similarity(np.concatenate([predict_001, predict_001, predict_002, predict_002]), np.concatenate([predict_001, predict_002, predict_001, predict_002]))\n",
    "print(score_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_voice_features = np.concatenate([predict_002, predict_002, predict_001, predict_002, predict_002, predict_002, predict_002, predict_002])\n",
    "voice_features = np.concatenate([predict_001, predict_002, predict_002, predict_002, predict_002, predict_001])\n",
    "np.mean(np.dot(voice_features, ref_voice_features.T), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_neg = score_pos.copy()\n",
    "score_neg[0] = 1\n",
    "score_pos < score_neg, score_pos, score_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_001.shape) [0.89747113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cosine_similarity(predict_001, predict_002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for audio_file in audio_files:\n",
    "    mfcc = sample_from_mfcc(read_mfcc(audio_file, SAMPLE_RATE), NUM_FRAMES)\n",
    "    predict = model.m.predict(np.expand_dims(mfcc, axis=0))\n",
    "    score = batch_cosine_similarity(predict_001, predict)\n",
    "    scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores), len(audio_files), np.shape(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.87\n",
    "scores = np.squeeze(np.array(scores))\n",
    "audio_files = np.array(audio_files)\n",
    "spk1_files = audio_files[scores > thr]\n",
    "spk2_files = audio_files[scores <= thr]\n",
    "\n",
    "spk1_data, spk2_data = [], []\n",
    "for spk1_file in spk1_files:\n",
    "    sr, data = wavfile.read(spk1_file)\n",
    "    spk1_data.append(data)\n",
    "for spk2_file in spk2_files:\n",
    "    sr, data = wavfile.read(spk2_file)\n",
    "    spk2_data.append(data)\n",
    "spk1_data = np.concatenate(spk1_data)\n",
    "spk2_data = np.concatenate(spk2_data)\n",
    "\n",
    "wavfile.write('/home/repsmate/ceva1.wav', 16000, spk1_data.astype(np.int16))\n",
    "wavfile.write('/home/repsmate/ceva2.wav', 16000, spk1_data.astype(np.int16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aeabe26c23c46db5840f299a32ff254170fc811839c1b7ac6e69cf1c22c239b0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
